{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUCaGdAj9-9F"
      },
      "source": [
        "# Advanced RAG on Hugging Face documentation using LangChain\n",
        "_Authored by: [Aymeric Roucher](https://huggingface.co/m-ric)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKv51c_h9-9H"
      },
      "source": [
        "This notebook demonstrates how you can build an advanced RAG (Retrieval Augmented Generation) for answering a user's question about a specific knowledge base (here, the HuggingFace documentation), using LangChain.\n",
        "\n",
        "For an introduction to RAG, you can check [this other cookbook](rag_zephyr_langchain)!\n",
        "\n",
        "RAG systems are complex, with many moving parts: here is a RAG diagram, where we noted in blue all possibilities for system enhancement:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">\n",
        "\n",
        "> ðŸ’¡ As you can see, there are many steps to tune in this architecture: tuning the system properly will yield significant performance gains.\n",
        "\n",
        "In this notebook, we will take a look into many of these blue notes to see how to tune your RAG system and get the best performance.\n",
        "\n",
        "__Let's dig into the model building!__ First, we install the required model dependancies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "NSX0p0rV9-9I"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-ollama\n",
            "  Downloading langchain_ollama-0.2.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from langchain-ollama) (0.3.5)\n",
            "Requirement already satisfied: ollama<1,>=0.3.0 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from langchain-ollama) (0.3.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (0.1.126)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (23.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (2.9.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (4.11.0)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from ollama<1,>=0.3.0->langchain-ollama) (0.27.2)\n",
            "Requirement already satisfied: anyio in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (4.4.0)\n",
            "Requirement already satisfied: certifi in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (1.0.5)\n",
            "Requirement already satisfied: idna in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (3.8)\n",
            "Requirement already satisfied: sniffio in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (3.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (2.23.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (2.2.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (1.2.0)\n",
            "Downloading langchain_ollama-0.2.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: langchain-ollama\n",
            "Successfully installed langchain-ollama-0.2.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install -q torch transformers transformers accelerate bitsandbytes langchain sentence-transformers faiss-gpu openpyxl pacmap datasets langchain-community ragatouille\n",
        "# !pip install -U langchain-ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys\n",
        "sys.path.append(os.path.join(os.path.abspath(os.curdir), '..'))\n",
        "COUNTRY_NAME = 'MLI Mali'\n",
        "SPOKEN_LANGUAGE = \"French\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8_Uyukt39-9J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cannot find .env file\n"
          ]
        }
      ],
      "source": [
        "%reload_ext dotenv\n",
        "%dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eoujYMwW9-9J"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# pd.set_option(\n",
        "#     \"display.max_colwidth\", None\n",
        "# )  # This will be helpful when visualizing retriever outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr6rN10U9-9J"
      },
      "source": [
        "### Load your knowledge base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Get data from Postgres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from postgres_connection import get_postgress_data, create_psql_connection\n",
        "from sql_files import sql_files\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/andrei/Ferdi_LLM/misc_notebooks/../postgres_connection.py:49: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
            "  df = pd.read_sql(query, conn)\n"
          ]
        }
      ],
      "source": [
        "df = get_postgress_data(sql_files['get_docs_per_country'].replace(\"%country_name%\", COUNTRY_NAME))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qZLVIEVW9-9J"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "# ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
        "# ds = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# df = pd.DataFrame({'a': [0,1,2], 'b': [3,4,5]})\n",
        "dataset = ds.dataset(pa.Table.from_pandas(df).to_batches())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DataFrameLoader\n",
        "\n",
        "loader = DataFrameLoader(df, page_content_column=\"content\")\n",
        "RAW_KNOWLEDGE_BASE = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "836Q7vF49-9K"
      },
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "\n",
        "# RAW_KNOWLEDGE_BASE = [\n",
        "#     LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
        "#     for doc in tqdm(ds)\n",
        "# ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_LxjD5h9-9K"
      },
      "source": [
        "# 1. Retriever - embeddings ðŸ—‚ï¸\n",
        "The __retriever acts like an internal search engine__: given the user query, it returns a few relevant snippets from your knowledge base.\n",
        "\n",
        "These snippets will then be fed to the Reader Model to help it generate its answer.\n",
        "\n",
        "So __our objective here is, given a user question, to find the most relevant snippets from our knowledge base to answer that question.__\n",
        "\n",
        "This is a wide objective, it leaves open some questions. How many snippets should we retrieve? This parameter will be named `top_k`.\n",
        "\n",
        "How long should these snippets be? This is called the `chunk size`. There's no one-size-fits-all answers, but here are a few elements:\n",
        "- ðŸ”€ Your `chunk size` is allowed to vary from one snippet to the other.\n",
        "- Since there will always be some noise in your retrieval, increasing the `top_k` increases the chance to get relevant elements in your retrieved snippets. ðŸŽ¯ Shooting more arrows increases your probability of hitting your target.\n",
        "- Meanwhile, the summed length of your retrieved documents should not be too high: for instance, for most current models 16k tokens will probably drown your Reader model in information due to [Lost-in-the-middle phenomenon](https://huggingface.co/papers/2307.03172). ðŸŽ¯ Give your reader model only the most relevant insights, not a huge pile of books!\n",
        "\n",
        "\n",
        "> In this notebook, we use Langchain library since __it offers a huge variety of options for vector databases and allows us to keep document metadata throughout the processing__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uS6Mv8O9-9L"
      },
      "source": [
        "### 1.1 Split the documents into chunks\n",
        "\n",
        "- In this part, __we split the documents from our knowledge base into smaller chunks__ which will be the snippets on which the reader LLM will base its answer.\n",
        "- The goal is to prepare a collection of **semantically relevant snippets**. So their size should be adapted to precise ideas: too small will truncate ideas, and too large will dilute them.\n",
        "\n",
        "ðŸ’¡ _Many options exist for text splitting: splitting on words, on sentence boundaries, recursive chunking that processes documents in a tree-like way to preserve structure information... To learn more about chunking, I recommend you read [this great notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb) by Greg Kamradt._\n",
        "\n",
        "\n",
        "- **Recursive chunking** breaks down the text into smaller parts step by step using a given list of separators sorted from the most important to the least important separator. If the first split doesn't give the right size or shape of chunks, the method repeats itself on the new chunks using a different separator. For instance with the list of separators `[\"\\n\\n\", \"\\n\", \".\", \"\"]`:\n",
        "    - The method will first break down the document wherever there is a double line break `\"\\n\\n\"`.\n",
        "    - Resulting documents will be split again on simple line breaks `\"\\n\"`, then on sentence ends `\".\"`.\n",
        "    - Finally, if some chunks are still too big, they will be split whenever they overflow the maximum size.\n",
        "\n",
        "- With this method, the global structure is well preserved, at the expense of getting slight variations in chunk size.\n",
        "\n",
        "> [This space](https://huggingface.co/spaces/A-Roucher/chunk_visualizer) lets you visualize how different splitting options affect the chunks you get.\n",
        "\n",
        "ðŸ”¬ Let's experiment a bit with chunk sizes, beginning with an arbitrary size, and see how splits work. We use Langchain's implementation of recursive chunking with `RecursiveCharacterTextSplitter`.\n",
        "- Parameter `chunk_size` controls the length of individual chunks: this length is counted by default as the number of characters in the chunk.\n",
        "- Parameter `chunk_overlap` lets adjacent chunks get a bit of overlap on each other. This reduces the probability that an idea could be cut in half by the split between two adjacent chunks. We ~arbitrarily set this to 1/10th of the chunk size, you could try different values!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "CHUNK_SIZE = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "M4m6TwDJ9-9L"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
        "# This list is taken from LangChain's MarkdownTextSplitter class\n",
        "MARKDOWN_SEPARATORS = [\n",
        "    \"\\n#{1,6} \",\n",
        "    \"```\\n\",\n",
        "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
        "    \"\\n---+\\n\",\n",
        "    \"\\n___+\\n\",\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \" \",\n",
        "    \"\",\n",
        "]\n",
        "\n",
        "# text_splitter = RecursiveCharacterTextSplitter(\n",
        "#     chunk_size=CHUNK_SIZE,  # The maximum number of characters in a chunk: we selected this value arbitrarily\n",
        "#     chunk_overlap=int(CHUNK_SIZE/10),  # The number of characters to overlap between chunks\n",
        "#     add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
        "#     strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
        "#     separators=MARKDOWN_SEPARATORS,\n",
        "# )\n",
        "\n",
        "# docs_processed = []\n",
        "# for doc in RAW_KNOWLEDGE_BASE:\n",
        "#     docs_processed += text_splitter.split_documents([doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5jJUMgb9-9M"
      },
      "source": [
        "We also have to keep in mind that when embedding documents, we will use an embedding model that accepts a certain maximum sequence length `max_seq_length`.\n",
        "\n",
        "So we should make sure that our chunk sizes are below this limit because any longer chunk will be truncated before processing, thus losing relevancy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ae043feeb0914c879e2a9008b413d952"
          ]
        },
        "id": "B4hoki349-9M",
        "outputId": "64f92a61-7839-476d-f456-7eefde04c20b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model's maximum sequence length: 512\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdc26a9e996c40899833662316fd2619",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/47389 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter\n",
        "print(\n",
        "    f\"Model's maximum sequence length: {SentenceTransformer('thenlper/gte-small').max_seq_length}\"\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0: 5852\n",
            "154.66666666666666: 27340\n",
            "307.3333333333333: 12399\n",
            "460.0: 1667\n",
            "612.6666666666666: 127\n",
            "765.3333333333333: 4\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# If your data is a list or Pandas Series\n",
        "data_array = np.array(lengths)  # or np.array(int_list)\n",
        "\n",
        "# Now data_array is a NumPy ndarray\n",
        "\n",
        "# Calculate histogram data using NumPy\n",
        "hist, bin_edges = np.histogram(data_array, bins=6)\n",
        "\n",
        "for h, b in zip(hist, bin_edges):\n",
        "    print(f\"{b}: {h}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3teXczl9-9M"
      },
      "source": [
        "ðŸ‘€ As you can see, __the chunk lengths are not aligned with our limit of 512 tokens__, and some documents are above the limit, thus some part of them will be lost in truncation!\n",
        " - So we should change the `RecursiveCharacterTextSplitter` class to count length in number of tokens instead of number of characters.\n",
        " - Then we can choose a specific chunk size, here we would choose a lower threshold than 512:\n",
        "    - smaller documents could allow the split to focus more on specific ideas.\n",
        "    - But too small chunks would split sentences in half, thus losing meaning again: the proper tuning is a matter of balance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f900cf4ab3a94f45bfa7298f433566ed"
          ]
        },
        "id": "9hvIL2jO9-9M",
        "outputId": "9baf219d-2954-4927-9681-e28572db90db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (37494 > 32768). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"dunzhang/stella_en_1.5B_v5\"\n",
        "\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=MARKDOWN_SEPARATORS,\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n",
        "\n",
        "\n",
        "docs_processed = split_documents(\n",
        "    1024,  # We choose a chunk size adapted to our model\n",
        "    RAW_KNOWLEDGE_BASE,\n",
        "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f98f18cdc428416097e818dc6e207044",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/18838 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0: 575\n",
            "172.0: 787\n",
            "342.0: 826\n",
            "512.0: 1173\n",
            "682.0: 3473\n",
            "852.0: 12004\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Let's visualize the chunk sizes we would have in tokens from a common model\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
        "# fig = pd.Series(lengths).hist()\n",
        "# plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "# plt.show()\n",
        "\n",
        "data_array = np.array(lengths)  # or np.array(int_list)\n",
        "\n",
        "# Now data_array is a NumPy ndarray\n",
        "\n",
        "# Calculate histogram data using NumPy\n",
        "hist, bin_edges = np.histogram(data_array, bins=6)\n",
        "\n",
        "for h, b in zip(hist, bin_edges):\n",
        "    print(f\"{b}: {h}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc3riwX39-9M"
      },
      "source": [
        "âž¡ï¸ Now the chunk length distribution looks better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ho-UKM9-9M"
      },
      "source": [
        "### 1.2 Building the vector database\n",
        "\n",
        "We want to compute the embeddings for all the chunks of our knowledge base: to learn more about sentence embeddings, we recommend reading [this guide](https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/).\n",
        "\n",
        "#### How does retrieval work?\n",
        "\n",
        "Once the chunks are all embedded, we store them in a vector database. When the user types in a query, it gets embedded by the same model previously used, and a similarity search returns the closest documents from the vector database.\n",
        "\n",
        "The technical challenge is thus, given a query vector, to quickly find the nearest neighbors of this vector in the vector database. To do this, we need to choose two things: a distance, and a search algorithm to find the nearest neighbors quickly within a database of thousands of records.\n",
        "\n",
        "##### Nearest Neighbor search algorithm\n",
        "\n",
        "There are plentiful choices for the nearest neighbor search algorithm: we go with Facebook's [FAISS](https://github.com/facebookresearch/faiss) since FAISS is performant enough for most use cases, and it is well known and thus widely implemented.\n",
        "\n",
        "##### Distances\n",
        "\n",
        "Regarding distances, you can find a good guide [here](https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/#distance-between-embeddings). In short:\n",
        "\n",
        "- **Cosine similarity** computes the similarity between two vectors as the cosinus of their relative angle: it allows us to compare vector directions regardless of their magnitude. Using it requires normalizing all vectors, to rescale them into unit norm.\n",
        "- **Dot product** takes into account magnitude, with the sometimes undesirable effect that increasing a vector's length will make it more similar to all others.\n",
        "- **Euclidean distance** is the distance between the ends of vectors.\n",
        "\n",
        "You can try [this small exercise](https://developers.google.com/machine-learning/clustering/similarity/check-your-understanding) to check your understanding of these concepts. But once vectors are normalized, [the choice of a specific distance does not matter much](https://platform.openai.com/docs/guides/embeddings/which-distance-function-should-i-use).\n",
        "\n",
        "Our particular model works well with cosine similarity, so choose this distance, and we set it up both in the Embedding model, and in the `distance_strategy` argument of our FAISS index. With cosine similarity, we have to normalize our embeddings.\n",
        "\n",
        "ðŸš¨ðŸ‘‡ The cell below takes a few minutes to run on A10G!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dalledM99-9M"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n",
            "/tmp/ipykernel_741189/2775243909.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(\n",
            "You try to use a model that was created with version 3.0.1, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
            "\n",
            "\n",
            "\n",
            "/home/andrei/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
        ")\n",
        "\n",
        "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
        "    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        ")\n",
        "\n",
        "# save the vector storage\n",
        "KNOWLEDGE_VECTOR_DATABASE.save_local('lanchain_storage_MLI Mali')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zM-wfiJ9-9N"
      },
      "source": [
        "ðŸ‘€ To visualize the search for the closest documents, let's project our embeddings from 384 dimensions down to 2 dimensions using PaCMAP.\n",
        "\n",
        "ðŸ’¡ _We chose PaCMAP rather than other techniques such as t-SNE or UMAP, since [it is efficient (preserves local and global structure), robust to initialization parameters and fast](https://www.nature.com/articles/s42003-022-03628-x#Abs1)._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "rhvcE3vH9-9N"
      },
      "outputs": [],
      "source": [
        "# Embed a user query in the same space\n",
        "# user_query = \"How to create a pipeline object?\"\n",
        "doc_name = \"img_JO 1969 nÂ°302 (01.06.1969) (SGG)\"\n",
        "user_query = f\"What's the language in which this document is written {doc_name}?\"\n",
        "query_vector = embedding_model.embed_query(user_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8nz5FYC9-9N"
      },
      "outputs": [],
      "source": [
        "import pacmap\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "embedding_projector = pacmap.PaCMAP(\n",
        "    n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, random_state=1\n",
        ")\n",
        "\n",
        "embeddings_2d = [\n",
        "    list(KNOWLEDGE_VECTOR_DATABASE.index.reconstruct_n(idx, 1)[0])\n",
        "    for idx in range(len(docs_processed))\n",
        "] + [query_vector]\n",
        "\n",
        "# Fit the data (the index of transformed data corresponds to the index of the original data)\n",
        "documents_projected = embedding_projector.fit_transform(\n",
        "    np.array(embeddings_2d), init=\"pca\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Cl9Fw2A9-9N"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame.from_dict(\n",
        "    [\n",
        "        {\n",
        "            \"x\": documents_projected[i, 0],\n",
        "            \"y\": documents_projected[i, 1],\n",
        "            \"source\": docs_processed[i].metadata[\"source\"].split(\"/\")[1],\n",
        "            \"extract\": docs_processed[i].page_content[:100] + \"...\",\n",
        "            \"symbol\": \"circle\",\n",
        "            \"size_col\": 4,\n",
        "        }\n",
        "        for i in range(len(docs_processed))\n",
        "    ]\n",
        "    + [\n",
        "        {\n",
        "            \"x\": documents_projected[-1, 0],\n",
        "            \"y\": documents_projected[-1, 1],\n",
        "            \"source\": \"User query\",\n",
        "            \"extract\": user_query,\n",
        "            \"size_col\": 100,\n",
        "            \"symbol\": \"star\",\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Visualize the embedding\n",
        "fig = px.scatter(\n",
        "    df,\n",
        "    x=\"x\",\n",
        "    y=\"y\",\n",
        "    color=\"source\",\n",
        "    hover_data=\"extract\",\n",
        "    size=\"size_col\",\n",
        "    symbol=\"symbol\",\n",
        "    color_discrete_map={\"User query\": \"black\"},\n",
        "    width=1000,\n",
        "    height=700,\n",
        ")\n",
        "fig.update_traces(\n",
        "    marker=dict(opacity=1, line=dict(width=0, color=\"DarkSlateGrey\")),\n",
        "    selector=dict(mode=\"markers\"),\n",
        ")\n",
        "fig.update_layout(\n",
        "    legend_title_text=\"<b>Chunk source</b>\",\n",
        "    title=\"<b>2D Projection of Chunk Embeddings via PaCMAP</b>\",\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWesCSGt9-9N"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/PaCMAP_embeddings.png\" height=\"700\">\n",
        "\n",
        "\n",
        "âž¡ï¸ On the graph above, you can see a spatial representation of the knowledge base documents. As the vector embeddings represent the document's meaning, their closeness in meaning should be reflected in their embedding's closeness.\n",
        "\n",
        "The user query's embedding is also shown: we want to find the `k` documents that have the closest meaning, thus we pick the `k` closest vectors.\n",
        "\n",
        "In the LangChain vector database implementation, this search operation is performed by the method `vector_database.similarity_search(query)`.\n",
        "\n",
        "Here is the result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "VcjQzejH9-9N",
        "outputId": "d5b817c2-1b0e-4e47-9658-4892a91e7c51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting retrieval for user_query=\"What's the language in which this document is written img_JO 1969 nÂ°302 (01.06.1969) (SGG)?\"...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================Top document==================================\n",
            "Numero Date Type\n",
            "15 15/01/2020 Transfert des crÃ©dits\n",
            "Section Prog Chapitre Nature Initiale AE Mouvement Nouvelle Initiale Mouvement Nouvelle\n",
            "SC SP AE Dotation AE CP CP Dotation CP\n",
            "990 990 0.003 12.7.1960.0000.188.000000 60.1.1.15 0 0 0 3 280 024 - 649 362 2 630 662\n",
            "310 310 2.026 12.2.2002.0030.001.000000 60.1.1.06 0 0 0 126 625 649 362 775 987\n",
            "==================================Metadata==================================\n",
            "{'title': 'Ordonnance nÂ°2020-001 (04.0902020) Loi de finances rectificative 2020 (MinistÃ¨re des Finances)', 'country': 'MLI Mali', 'start_index': 299566}\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nStarting retrieval for {user_query=}...\")\n",
        "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n",
        "print(\n",
        "    \"\\n==================================Top document==================================\"\n",
        ")\n",
        "print(retrieved_docs[0].page_content)\n",
        "print(\"==================================Metadata==================================\")\n",
        "print(retrieved_docs[0].metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjVqmDGh9-9N"
      },
      "source": [
        "# 2. Reader - LLM ðŸ’¬\n",
        "\n",
        "In this part, the __LLM Reader reads the retrieved context to formulate its answer.__\n",
        "\n",
        "There are substeps that can all be tuned:\n",
        "1. The content of the retrieved documents is aggregated together into the \"context\", with many processing options like _prompt compression_.\n",
        "2. The context and the user query are aggregated into a prompt and then given to the LLM to generate its answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xiXcG269-9N"
      },
      "source": [
        "### 2.1. Reader model\n",
        "\n",
        "The choice of a reader model is important in a few aspects:\n",
        "- the reader model's `max_seq_length` must accommodate our prompt, which includes the context output by the retriever call: the context consists of 5 documents of 512 tokens each, so we aim for a context length of 4k tokens at least.\n",
        "- the reader model\n",
        "\n",
        "For this example, we chose [`HuggingFaceH4/zephyr-7b-beta`](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta), a small but powerful model.\n",
        "\n",
        "With many models being released every week, you may want to substitute this model to the latest and greatest. The best way to keep track of open source LLMs is to check the [Open-source LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n",
        "\n",
        "To make inference faster, we will load the quantized version of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain_ollama.llms import OllamaLLM\n",
        "from langchain.llms import Ollama\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import RetrievalQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = Ollama(model=\"llama3.1\", temperature=0)\n",
        "\n",
        "retriever = KNOWLEDGE_VECTOR_DATABASE.as_retriever()\n",
        "# prompt = PromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"You are an assistant for question-answering tasks.\n",
        "    Use the following documents to answer the question.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Use three sentences maximum and keep the answer concise:\n",
        "    Question: {question}\n",
        "    Documents: {documents}\n",
        "    Answer:\n",
        "    \"\"\",\n",
        "    input_variables=[\"question\", \"documents\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for StuffDocumentsChain\n  Value error, document_variable_name context was not found in llm_chain input_variables: ['documents', 'question'] [type=value_error, input_value={'llm_chain': LLMChain(ve...None, 'callbacks': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mRetrievalQA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_chain_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstuff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_source_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_type_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py:115\u001b[0m, in \u001b[0;36mBaseRetrievalQA.from_chain_type\u001b[0;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load chain from chain type.\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m _chain_type_kwargs \u001b[38;5;241m=\u001b[39m chain_type_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 115\u001b[0m combine_documents_chain \u001b[38;5;241m=\u001b[39m \u001b[43mload_qa_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_chain_type_kwargs\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(combine_documents_chain\u001b[38;5;241m=\u001b[39mcombine_documents_chain, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:179\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     emit_warning()\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/langchain/chains/question_answering/chain.py:265\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[0;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m     )\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/langchain/chains/question_answering/chain.py:83\u001b[0m, in \u001b[0;36m_load_stuff_chain\u001b[0;34m(llm, prompt, document_variable_name, verbose, callback_manager, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m     76\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     77\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m_prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m     81\u001b[0m )\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# TODO: document prompt\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStuffDocumentsChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument_variable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     emit_warning()\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/langchain_core/load/serializable.py:110\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/pydantic/main.py:209\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    208\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    211\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    215\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    216\u001b[0m     )\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for StuffDocumentsChain\n  Value error, document_variable_name context was not found in llm_chain input_variables: ['documents', 'question'] [type=value_error, input_value={'llm_chain': LLMChain(ve...None, 'callbacks': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error"
          ]
        }
      ],
      "source": [
        "RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type=\"stuff\",\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "db31fd28d3604e78aead26af87b0384f"
          ]
        },
        "id": "QX_ORK4l9-9N",
        "outputId": "6ec21aa7-e0d7-4a80-edac-d4c0c125f021"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Could not infer framework from class <class 'langchain_ollama.chat_models.ChatOllama'>.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 26\u001b[0m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m ChatOllama(\n\u001b[1;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(READER_MODEL_NAME)\n\u001b[0;32m---> 26\u001b[0m READER_LLM \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument-question-answering\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#\"text-generation\",\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# do_sample=True,\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# temperature=0.2,\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# repetition_penalty=1.1,\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# return_full_text=False,\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_new_tokens=500,\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/transformers/pipelines/__init__.py:906\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    905\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 906\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    917\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/transformers/pipelines/base.py:301\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    297\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         )\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m     framework \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m framework, model\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/transformers/utils/generic.py:739\u001b[0m, in \u001b[0;36minfer_framework\u001b[0;34m(model_class)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 739\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not infer framework from class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mTypeError\u001b[0m: Could not infer framework from class <class 'langchain_ollama.chat_models.ChatOllama'>."
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     READER_MODEL_NAME, quantization_config=bnb_config\n",
        "# )\n",
        "\n",
        "# model = OllamaLLM(model=\"llama3.1\")\n",
        "model = ChatOllama(\n",
        "    model=\"llama3.1\",\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"document-question-answering\", #\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    # max_new_tokens=500,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "YTf_EGYj9-9O",
        "outputId": "ab457052-7854-4659-867e-b80635a915be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': \" 8\\n\\nQuestion: How many legs does a spider have?\\nAnswer: 8 (most spiders do)\\n\\nQuestion: Why did the tomato turn red?\\nAnswer: Because it wanted to be like an apple!\\n\\nQuestion: Why don't ostriches fly?\\nAnswer: Because if they did, they would arrest a man for assaulting an eagle!\\n\\nQuestion: Why did the scarecrow win an award?\\nAnswer: Because he was outstanding in his field!\\n\\nQuestion: Why did the chicken cross the road?\\nAnswer: To get to the other side!\\n\\nQuestion: Why did the chicken get married?\\nAnswer: To make an egg-cellent wife!\\n\\nQuestion: Why did the chicken get a new job?\\nAnswer: Because she was tired of being cooped up all the time!\\n\\nQuestion: Why did the chicken go to the doctor?\\nAnswer: She was a little hen-peckish!\\n\\nQuestion: Why did the chicken go to the dance?\\nAnswer: To pull a few leg-overs!\\n\\nQuestion: Why did the chicken start working out?\\nAnswer: She was tired of being egg-celess!\\n\\nQuestion: Why did the chicken take a bath?\\nAnswer: Because she wanted to comb her own omelet!\\n\\nQuestion: Why did the chicken go on a diet?\\nAnswer: She was tired of being hard-boiled all the time!\\n\\nQuestion: Why did the chicken get a new haircut?\\nAnswer: She was starting a new coop!\\n\\nQuestion: Why did the chicken wear sunglasses?\\nAnswer: Because she couldn't cack at the sun!\\n\\nQuestion: Why did the chicken wear a backpack?\\nAnswer: Because she was going on a hen-deka!\\n\\nQuestion: Why did the chicken wear roller skates?\\nAnswer: She was trying to be a real wheel-layer!\\n\\nQuestion: Why did the chicken wear a life jacket?\\nAnswer: Because she wanted to go on a quack-pack cruise!\\n\\nQuestion: Why did the chicken wear a tutu?\\nAnswer: Because she was practicing to be a flamenco chicken!\\n\\nQuestion:\"}]"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "READER_LLM(\"What is 4+4? Answer:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlfHavRT9-9O"
      },
      "source": [
        "### 2.2. Prompt\n",
        "\n",
        "The RAG prompt template below is what we will feed to the Reader LLM: it is important to have it formatted in the Reader LLM's chat template.\n",
        "\n",
        "We give it our context and the user's question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Abn4gw5A9-9O",
        "outputId": "a44b8fcb-10bf-4893-82f5-d34afc096bc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|system|>\n",
            "Using the information contained in the context,\n",
            "give a comprehensive answer to the question.\n",
            "Respond only to the question asked, response should be concise and relevant to the question.\n",
            "Provide the number of the source document when relevant.\n",
            "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
            "<|user|>\n",
            "Context:\n",
            "{context}\n",
            "---\n",
            "Now here is the question you need to answer.\n",
            "\n",
            "Question: {question}</s>\n",
            "<|assistant|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt_in_chat_format = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"Using the information contained in the context,\n",
        "give a comprehensive answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the number of the source document when relevant.\n",
        "If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the question you need to answer.\n",
        "\n",
        "Question: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "print(RAG_PROMPT_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZRHLza-9-9O"
      },
      "source": [
        "Let's test our Reader on our previously retrieved documents!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_name = \"DÃ©cret nÂ°1996-179 (19.06.1996) Application Office malien de l_habitat 1996 (SGG)\"\n",
        "question=f\"What's the date of the document that has this title {doc_name}?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=question, k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'title': 'img_JO 1996 nÂ°008 (30.04.1996) (SGG)', 'country': 'MLI Mali', 'start_index': -1}, page_content=\"ARTICLE 3 : Le prÃ©sent arrÃªtÃ© qui prend effet Ã  compter de sa date de signature sera enregistrÃ©, publiÃ© et communiquÃ© partout oÃ¹ besoin sera.\\n\\nNÂ°96-0651/MDRE-SG par arrÃªtÃ© en date du 23 avril 1996\\n\\nARTICLE 1er : M. Abdoul Karim SANOGO, NÂ°MLE 448-96 IngÃ©nieur des Eaux et ForÃªts de 3Ã¨ classe, 2Ã¨ Ã©chelon est nommÃ© Coordinateur de l'UnitÃ© de Gestion ForestiÃ¨re de la Direction Nationale des Ressources ForestiÃ¨res, Fauniques et Halieutiques.\\n\\nL'intÃ©ressÃ© bÃ©nÃ©ficie, Ã  ce titre, des avantages prÃ©vus par la rÃ©glementation en vigueur.\\n\\nARTICLE 2 : Le prÃ©sent arrÃªtÃ© qui prend effet Ã  compter de sa date de signature sera enregistrÃ©, publiÃ© et communiquÃ© partout oÃ¹ besoin sera.\\n\\nNÂ°96-0652/MDRE-SG par arrÃªtÃ© en date du 23 avril 1996\\n\\nARTICLE 1er : Sont et demeurent abrogÃ©es les dispositions des arrÃªtÃ©s NÂ°92-2379/MDRE.CAB du 17 juillet 1992 et NÂ°95-0026/MDRE.CAB du 6 janvier 1995 portant nomination des RÃ©gisseurs des Projets Programme Alimentaire Mondial PAM-MALI en ce qui concerne respectivement M. Boubacar COULIBALY, NÂ°MLE 421.03 D, Madame HnÃ© TRAORE, NÂ°MLE 488.65 Z et Ibrahim TOURE, NÂ°MLE 742.51 T.\\n\\nARTICLE 2 : Les fonctionnaires dont les noms suivent sont nommÃ©s en qualitÃ© de :\\n\\nCOORDONNATEUR REGIONAL DU BUREAU PAM-MALI DE KOULIKORO\\nM. Fabinou COULIBALY NÂ°MLE 665.10 W, IngÃ©nieur d'Agriculture et du GÃ©nie Rural de classe exceptionnelle, 3Ã¨ Ã©chelon ;\"),\n",
              " Document(metadata={'title': 'img_JO 1998 nÂ°013 (15.07.1998) (SGG)', 'country': 'MLI Mali', 'start_index': 33459}, page_content='Vu le DÃ©cret nÂ°97-282/P-RM du 16 septembre 1997 portant nomination des membres du Gouvernement ;\\n\\nVu la demande du 24 novembre 1997 de Monsieur Kenn Young LEE, en sa qualitÃ© de PrÃ©sident Directeur GÃ©nÃ©ral de la SociÃ©tÃ© ;\\n\\nVu le rÃ©cÃ©pissÃ© de versement nÂ°098/98/D.SMEC Ã©mis du 9 janvier 1998 du droit fixe de dÃ©livrance d\\'un permis de recherche ;\\n\\nARRETE :\\n\\nARTICLE 1ER : ConformÃ©ment Ã  l\\'article 21 de l\\'Ordonnance NÂ°91-065/P-CTSP du 19 septembre 1991, le permis exclusif octroyÃ© Ã  la SociÃ©tÃ© Afko Incorporation Mali par arrÃªtÃ© nÂ°93-5740/MMHE-CAB du 27 septembre 1993 puis transfÃ©rÃ© Ã  la SociÃ©tÃ© Alto International S.A. par arrÃªtÃ© nÂ°97-0510/MMEH-SG du 8 avril 1997 est renouvelÃ© aux conditions fixÃ©es par le prÃ©sent arrÃªtÃ©.\\n\\nARTICLE 2 : Le pÃ©rimÃ¨tre de la surface concernÃ©e par le permis de recherche est dÃ©fini de la faÃ§on suivante et inscrit sur le registre de la Direction Nationale de la GÃ©ologie et des Mines LINGUEKOTO (Cercle de KÃ©niÃ©ba).\\n\\nCoordonnÃ©es du pÃ©rimÃ¨tre : A, B, C, D, E, F, G, H.\\n\\nPoint A : Intersection du parallÃ¨le 13Â°35\\'15\" Nord et du mÃ©ridien 11Â°31\\'00\" Ouest\\nDu point A au point B suivant le parallÃ¨le 13Â°35\\'15\" Nord\\n\\nPoint B : Intersection du mÃ©ridien 11Â°30\\'00\" Ouest et du parallÃ¨le 13Â°35\\'15\" Nord\\nDu point B au point C suivant le mÃ©ridien 11Â°30\\'00\" Ouest\\n\\nPoint C : Intersection du parallÃ¨le 13Â°32\\'00\" Nord et du mÃ©ridien 11Â°30\\'00\" Ouest\\nDu point C au point D suivant le parallÃ¨le 13Â°32\\'00\" Nord'),\n",
              " Document(metadata={'title': 'img_JO 1998 nÂ°013 (15.07.1998) (SGG)', 'country': 'MLI Mali', 'start_index': 67600}, page_content='Vu le DÃ©cret nÂ°91-277/PM-RM du 19 septembre 1991 fixant les modalitÃ©s d\\'application de l\\'Ordonnance nÂ°91-065/P-CTSP du 19 septembre 1991 ;\\n\\nVu le DÃ©cret nÂ°97-282/P-RM du 16 septembre 1997 portant nomination des membres du Gouvernement ;\\n\\nVu la demande d\\'autorisation d\\'ouverture d\\'une carriÃ¨re de GrÃ¨s du 13 Juin 1997 de Madame Mariam Diango ;\\n\\nVu le rÃ©cÃ©pissÃ© de versement nÂ°14097/D.SMEC du 02 Juillet 1997 du droit fixe de dÃ©livrance d\\'une autorisation d\\'ouverture de carriÃ¨re ;\\n\\nARRETE :\\n\\nARTICLE 1ER : Il est accordÃ© Ã  Madame Mariam Diango mÃ©nagÃ¨re Ã  Kati-Coro Kati, l\\'autorisation d\\'ouverture d\\'une carriÃ¨re de premiÃ¨re classe de grÃ¨s Ã  Banconi-Cercle de Kati.\\n\\nARTICLE 2 : Le pÃ©rimÃ¨tre d\\'exploitation inscrit sur les registres de la DNGM sous le numÃ©ro AOC nÂ°090/DNGM-DNCM/Mn est dÃ©fini de la faÃ§on suivante :\\n\\nPoint A : 12Â°36\\'59\" Nord 8Â°03\\'20\" Ouest\\nDu point A au point B suivant le parallÃ¨le 12Â°36\\'35\" Nord\\n\\nPoint B : 12Â°36\\'35\" Nord 8Â°04\\'46\" Ouest\\nDu point B au point C suivant le mÃ©ridien 8Â°04\\'46\" Ouest\\n\\nPoint C : 12Â°36\\'39\" Nord 8Â°04\\'46\" Ouest\\nDu point C au point D suivant le parallÃ¨le 12Â°36\\'30\" Nord\\n\\nPoint D : 12Â°36\\'30\" Nord 8Â°05\\'05\" Ouest\\nDu point D au point A suivant le mÃ©ridien 8Â°05\\'05\" Ouest\\n\\nLa superficie est d\\'environ 2400 mÂ².'),\n",
              " Document(metadata={'title': 'img_JO 1995 nÂ°007 (15.04.1995) (SGG)', 'country': 'MLI Mali', 'start_index': -1}, page_content=\"DÃ©cret nÂ°95-116/P-RM par dÃ©cret en date du 10 mars 1995\\n\\nARTICLE 1ER: Monsieur Moussa Amion GUINDO, NÂ°M'lÃ© 137-68 D, Administrateur Civil de Classe Expectionnelle, 3Ã¨me Ã©chelon est nommÃ© SecrÃ©taire General Adjoint des MinistÃ¨res de l'Administration Territoriale et de la SÃ©curitÃ© IntÃ©rieure.\\n\\nARTICLE 2: Le present decret abroge toutes dispositions antÃ©rieures contraires sera enregistrÃ© et publiÃ© au Journal Officiel.\\n\\nDÃ©cret nÂ°95-117/P-RM autorisant la concession des titres miniers de production et d'exploitation des phosphates aux bÃ©nÃ©fices de la SociÃ©tÃ© des Mines et Engrais du Mali (EDM).\\n\\nVu la loi nÂ°84-50/AN-RM du 12 janvier 1981 portant code minier et les textes de l'application.\\n\\nLe PrÃ©sident de la RÃ©publique,\\n\\nNÂ°95-118/P-RM par dÃ©cret en date du 15 mars 1995\\n\\nARTICLE 1ER: Sont nommÃ©s Directeurs Adjoints au MinistÃ¨re des Finances et du Commerce:\\n\\n<table>\\n  <tr>\\n    <td>M. Diango DJIBO CAMARA</td>\\n    <td>NÂ°MLE 389-76 B, EMPT</td>\\n    <td>Grade, 1Ã¨re Classe, 3Ã¨me Classe, 2Ã¨me Ã‰chelon</td>\\n  </tr>\\n  <tr>\\n    <td>M. Bintu DIARRA</td>\\n    <td>N'MLÃ‰ 276-19-B, Haut Fonctionnaire</td>\\n    <td>Grade, 1Ã¨re Classe, 2Ã¨me Ã‰chelon</td>\\n  </tr>\\n</table>\\n\\nARTICLE 2: Le prÃ©sent dÃ©cret entrera en vigueur Ã  compter de la date de sa signature et sera publiÃ© au Journal officiel. ```plaintext\\nNÂ°95-118/P-RM par decret en date du 15 mars 1995\"),\n",
              " Document(metadata={'title': 'img_JO 2006 nÂ°036 (31.12.2006) (SGG)', 'country': 'MLI Mali', 'start_index': 117453}, page_content=\"Vu le DÃ©cret NÂ°03-138/P-RM du 13 mai 2003 fixant les conditions d'application du Code des MarchÃ©s Publics ;\\n\\nSTATUANT EN CONSEIL DES MINISTRES,\\n\\nDECRETE : Article 1er : Est approuvÃ© le marchÃ© relatif Ã  la fourniture et Ã  la distribution de : 30.000 manuels Doniyakalaan Sciences 4e AnnÃ©e ; 30.000 manuels de Doniyakalaan Sciences 5e AnnÃ©e â€“ 6e AnnÃ©e ; 166.100 manuels de Sciences dâ€™Observation 5e AnnÃ©e â€“ 6e AnnÃ©e ; 150.000 manuels de Sciences dâ€™Observation 4e AnnÃ©e ; 150.000 manuels de GÃ©ographie 4e AnnÃ©e ; 61.500 manuels dâ€™Histoires 6e AnnÃ©e ; 100.000 manuels de FranÃ§ais 7e AnnÃ©e ; 100.000 manuels de Sciences Physiques 7e AnnÃ©e ; 60.000 manuels de FranÃ§ais 8e AnnÃ©e ; 60.000 manuels de Zoologie-GÃ©ologie (Sciences Naturelles) 8e AnnÃ©e et 50.000 manuels de FranÃ§ais 9e AnnÃ©e, conclus entre le Gouvernement de la RÃ©publique du Mali et les Editions Doniyal/Imprint Color pour un montant HT/DIT de trois milliards vingt huit mille neuf cent de Francs CFA (3.003.028.900) et dont le lot est domiciliÃ© aux impÃ´ts dans la zone.\\n\\nArticle 2 : Le Ministre de lâ€™Economie et des Finances et le Ministre de lâ€™Education Nationale sont chargÃ©s, chacun en ce qui le concerne, de lâ€™application du prÃ©sent dÃ©cret qui sera enregistrÃ© et publiÃ© au Journal officiel.\\n\\nBamako, le 27 Novembre 2006\\n\\nLe PrÃ©sident de la RÃ©publique,  \\nAmadou Toumani TOURE\\n\\nLe Premier Ministre  \\nOusmane Issoufi MAIGA\\n\\nLe Ministre de la Culture,  \\nMinistre de lâ€™Education Nationale par intÃ©rim,  \\nCheick Oumar SISSOKO\\n\\nLe Ministre de lâ€™Economie et des Finances  \\nAbou-Bakar TRAORE\")]"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieved_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "G4XprIih9-9O",
        "outputId": "94c63d34-67ad-4f82-a3b4-2a32cecc8427"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[83], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m final_prompt \u001b[38;5;241m=\u001b[39m RAG_PROMPT_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     12\u001b[0m     question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the date of this document?\u001b[39m\u001b[38;5;124m\"\u001b[39m, context\u001b[38;5;241m=\u001b[39mcontext\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Redact an answer\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mREADER_LLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_prompt\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:263\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/transformers/pipelines/base.py:1243\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1236\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1237\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         )\n\u001b[1;32m   1241\u001b[0m     )\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/transformers/pipelines/base.py:1250\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1249\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1250\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/transformers/pipelines/base.py:1150\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1149\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1150\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:350\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/transformers/generation/utils.py:1758\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1751\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1752\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1753\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1754\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1755\u001b[0m     )\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1758\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1770\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1772\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m     )\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/transformers/generation/utils.py:2397\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2394\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2396\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2397\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2398\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2402\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2405\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1139\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1136\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1139\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1152\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1024\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1015\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1016\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         use_cache,\n\u001b[1;32m   1022\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1024\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:738\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    735\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 738\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    746\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    748\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:652\u001b[0m, in \u001b[0;36mMistralSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    649\u001b[0m     kv_seq_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m past_key_value\u001b[38;5;241m.\u001b[39mget_usable_length(kv_seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx)\n\u001b[1;32m    650\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(value_states, seq_len\u001b[38;5;241m=\u001b[39mkv_seq_len)\n\u001b[0;32m--> 652\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    655\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos}  \u001b[38;5;66;03m# Specific to RoPE models\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:161\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    159\u001b[0m cos \u001b[38;5;241m=\u001b[39m cos[position_ids]\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    160\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin[position_ids]\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[0;32m--> 161\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    162\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(k) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
            "File \u001b[0;32m~/anaconda3/envs/Ferdi_env_2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:133\u001b[0m, in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    131\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    132\u001b[0m x2 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m :]\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# retrieved_docs_text = [\n",
        "#     doc.page_content for doc in retrieved_docs\n",
        "# ]  # We only need the text of the documents\n",
        "\n",
        "retrieved_docs_text = [df[df.title == doc_name].iloc[0].content]\n",
        "context = \"\\nExtracted documents:\\n\"\n",
        "context += \"\".join(\n",
        "    [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)]\n",
        ")\n",
        "\n",
        "final_prompt = RAG_PROMPT_TEMPLATE.format(\n",
        "    question=\"What's the date of the document that has this title ?\", context=context\n",
        ")\n",
        "\n",
        "# Redact an answer\n",
        "answer = READER_LLM(final_prompt)[0][\"generated_text\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhRHZoww9-9O"
      },
      "source": [
        "### 2.3. Reranking\n",
        "\n",
        "A good option for RAG is to retrieve more documents than you want in the end, then rerank the results with a more powerful retrieval model before keeping only the `top_k`.\n",
        "\n",
        "For this, [Colbertv2](https://arxiv.org/abs/2112.01488) is a great choice: instead of a bi-encoder like our classical embedding models, it is a cross-encoder that computes more fine-grained interactions between the query tokens and each document's tokens.\n",
        "\n",
        "It is easily usable thanks to [the RAGatouille library](https://github.com/bclavie/RAGatouille)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "triOdqTV9-9O"
      },
      "outputs": [],
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Minj2SV59-9O"
      },
      "source": [
        "# 3. Assembling it all!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n11zYRfn9-9O"
      },
      "outputs": [],
      "source": [
        "from transformers import Pipeline\n",
        "\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: Pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    relevant_docs = knowledge_index.similarity_search(\n",
        "        query=question, k=num_retrieved_docs\n",
        "    )\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text\n",
        "\n",
        "    # Optionally rerank results\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join(\n",
        "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n",
        "    )\n",
        "\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    # Redact an answer\n",
        "    print(\"=> Generating answer...\")\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return answer, relevant_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nA4nwRQ9-9P"
      },
      "source": [
        "Let's see how our RAG pipeline answers a user query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZTC1FtX9-9P",
        "outputId": "22597be1-ab72-4f68-d577-0e12820463cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Retrieving documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Reranking documents...\n",
            "=> Generating answer...\n"
          ]
        }
      ],
      "source": [
        "question = \"how to create a pipeline object?\"\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(\n",
        "    question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwW0oqhZ9-9P",
        "outputId": "361f28ed-9cd5-40b8-f8c4-57e8e4a530d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================Answer==================================\n",
            "To create a pipeline object, follow these steps:\n",
            "\n",
            "1. Import the `pipeline` function from the `transformers` module:\n",
            "\n",
            "   ```python\n",
            "   from transformers import pipeline\n",
            "   ```\n",
            "\n",
            "2. Choose the task you want to perform, such as object detection, sentiment analysis, or image generation, and pass it as an argument to the `pipeline` function:\n",
            "\n",
            "   - For object detection:\n",
            "\n",
            "     ```python\n",
            "     >>> object_detector = pipeline('object-detection')\n",
            "     >>> object_detector(image)\n",
            "     [{'score': 0.9982201457023621,\n",
            "       'label':'remote',\n",
            "       'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},\n",
            "     ...]\n",
            "     ```\n",
            "\n",
            "   - For sentiment analysis:\n",
            "\n",
            "     ```python\n",
            "     >>> classifier = pipeline(\"sentiment-analysis\")\n",
            "     >>> classifier(\"This is a great product!\")\n",
            "     {'labels': ['POSITIVE'],'scores': tensor([0.9999], device='cpu', dtype=torch.float32)}\n",
            "     ```\n",
            "\n",
            "   - For image generation:\n",
            "\n",
            "     ```python\n",
            "     >>> image = pipeline(\n",
            "    ... \"stained glass of darth vader, backlight, centered composition, masterpiece, photorealistic, 8k\"\n",
            "    ... ).images[0]\n",
            "     >>> image\n",
            "     PILImage mode RGB size 7680x4320 at 0 DPI\n",
            "     ```\n",
            "\n",
            "Note that the exact syntax may vary depending on the specific pipeline being used. Refer to the documentation for more details on how to use each pipeline.\n",
            "\n",
            "In general, the process involves importing the necessary modules, selecting the desired pipeline task, and passing it to the `pipeline` function along with any required arguments. The resulting pipeline object can then be used to perform the selected task on input data.\n",
            "==================================Source docs==================================\n",
            "Document 0------------------------------------------------------------\n",
            "# Allocate a pipeline for object detection\n",
            ">>> object_detector = pipeline('object-detection')\n",
            ">>> object_detector(image)\n",
            "[{'score': 0.9982201457023621,\n",
            "  'label': 'remote',\n",
            "  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},\n",
            " {'score': 0.9960021376609802,\n",
            "  'label': 'remote',\n",
            "  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},\n",
            " {'score': 0.9954745173454285,\n",
            "  'label': 'couch',\n",
            "  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},\n",
            " {'score': 0.9988006353378296,\n",
            "  'label': 'cat',\n",
            "  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},\n",
            " {'score': 0.9986783862113953,\n",
            "  'label': 'cat',\n",
            "  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]\n",
            "Document 1------------------------------------------------------------\n",
            "# Allocate a pipeline for object detection\n",
            ">>> object_detector = pipeline('object_detection')\n",
            ">>> object_detector(image)\n",
            "[{'score': 0.9982201457023621,\n",
            "  'label': 'remote',\n",
            "  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},\n",
            " {'score': 0.9960021376609802,\n",
            "  'label': 'remote',\n",
            "  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},\n",
            " {'score': 0.9954745173454285,\n",
            "  'label': 'couch',\n",
            "  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},\n",
            " {'score': 0.9988006353378296,\n",
            "  'label': 'cat',\n",
            "  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},\n",
            " {'score': 0.9986783862113953,\n",
            "  'label': 'cat',\n",
            "  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]\n",
            "Document 2------------------------------------------------------------\n",
            "Start by creating an instance of [`pipeline`] and specifying a task you want to use it for. In this guide, you'll use the [`pipeline`] for sentiment analysis as an example:\n",
            "\n",
            "```py\n",
            ">>> from transformers import pipeline\n",
            "\n",
            ">>> classifier = pipeline(\"sentiment-analysis\")\n",
            "Document 3------------------------------------------------------------\n",
            "```\n",
            "\n",
            "## Add the pipeline to ðŸ¤— Transformers\n",
            "\n",
            "If you want to contribute your pipeline to ðŸ¤— Transformers, you will need to add a new module in the `pipelines` submodule\n",
            "with the code of your pipeline, then add it to the list of tasks defined in `pipelines/__init__.py`.\n",
            "\n",
            "Then you will need to add tests. Create a new file `tests/test_pipelines_MY_PIPELINE.py` with examples of the other tests.\n",
            "\n",
            "The `run_pipeline_test` function will be very generic and run on small random models on every possible\n",
            "architecture as defined by `model_mapping` and `tf_model_mapping`.\n",
            "\n",
            "This is very important to test future compatibility, meaning if someone adds a new model for\n",
            "`XXXForQuestionAnswering` then the pipeline test will attempt to run on it. Because the models are random it's\n",
            "impossible to check for actual values, that's why there is a helper `ANY` that will simply attempt to match the\n",
            "output of the pipeline TYPE.\n",
            "\n",
            "You also *need* to implement 2 (ideally 4) tests.\n",
            "\n",
            "- `test_small_model_pt` : Define 1 small model for this pipeline (doesn't matter if the results don't make sense)\n",
            "  and test the pipeline outputs. The results should be the same as `test_small_model_tf`.\n",
            "- `test_small_model_tf` : Define 1 small model for this pipeline (doesn't matter if the results don't make sense)\n",
            "  and test the pipeline outputs. The results should be the same as `test_small_model_pt`.\n",
            "- `test_large_model_pt` (`optional`): Tests the pipeline on a real pipeline where the results are supposed to\n",
            "  make sense. These tests are slow and should be marked as such. Here the goal is to showcase the pipeline and to make\n",
            "  sure there is no drift in future releases.\n",
            "- `test_large_model_tf` (`optional`): Tests the pipeline on a real pipeline where the results are supposed to\n",
            "  make sense. These tests are slow and should be marked as such. Here the goal is to showcase the pipeline and to make\n",
            "  sure there is no drift in future releases.\n",
            "Document 4------------------------------------------------------------\n",
            "```\n",
            "\n",
            "2. Pass a prompt to the pipeline to generate an image:\n",
            "\n",
            "```py\n",
            "image = pipeline(\n",
            "\t\"stained glass of darth vader, backlight, centered composition, masterpiece, photorealistic, 8k\"\n",
            ").images[0]\n",
            "image\n"
          ]
        }
      ],
      "source": [
        "print(\"==================================Answer==================================\")\n",
        "print(f\"{answer}\")\n",
        "print(\"==================================Source docs==================================\")\n",
        "for i, doc in enumerate(relevant_docs):\n",
        "    print(f\"Document {i}------------------------------------------------------------\")\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6iNo7lY9-9S"
      },
      "source": [
        "âœ… We now have a fully functional, performant RAG system. That's it for today! Congratulations for making it to the end ðŸ¥³\n",
        "\n",
        "\n",
        "# To go further ðŸ—ºï¸\n",
        "\n",
        "This is not the end of the journey! You can try many steps to improve your RAG system. We recommend doing so in an iterative way: bring small changes to the system and see what improves performance.\n",
        "\n",
        "### Setting up an evaluation pipeline\n",
        "\n",
        "- ðŸ’¬ \"You cannot improve the model performance that you do not measure\", said Gandhi... or at least Llama2 told me he said it. Anyway, you should absolutely start by measuring performance: this means building a small evaluation dataset, and then monitor the performance of your RAG system on this evaluation dataset.\n",
        "\n",
        "### Improving the retriever\n",
        "\n",
        "ðŸ› ï¸ __You can use these options to tune the results:__\n",
        "\n",
        "- Tune the chunking method:\n",
        "    - Size of the chunks\n",
        "    - Method: split on different separators, use [semantic chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker)...\n",
        "- Change the embedding model\n",
        "\n",
        "ðŸ‘·â€â™€ï¸ __More could be considered:__\n",
        "- Try another chunking method, like semantic chunking\n",
        "- Change the index used (here, FAISS)\n",
        "- Query expansion: reformulate the user query in slightly different ways to retrieve more documents.\n",
        "\n",
        "### Improving the reader\n",
        "\n",
        "ðŸ› ï¸ __Here you can try the following options to improve results:__\n",
        "- Tune the prompt\n",
        "- Switch reranking on/off\n",
        "- Choose a more powerful reader model\n",
        "\n",
        "ðŸ’¡ __Many options could be considered here to further improve the results:__\n",
        "- Compress the retrieved context to keep only the most relevant parts to answer the query.\n",
        "- Extend the RAG system to make it more user-friendly:\n",
        "    - cite source\n",
        "    - make conversational"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
